{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142d36c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting pdfminer.six==20251107 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-12.0.0-cp314-cp314-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.0.0-py3-none-win_amd64.whl.metadata (67 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20251107->pdfplumber)\n",
      "  Downloading charset_normalizer-3.4.4-cp314-cp314-win_amd64.whl.metadata (38 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20251107->pdfplumber)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber)\n",
      "  Downloading cffi-2.0.0-cp314-cp314-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber)\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.6 MB 13.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 12.6 MB/s  0:00:00\n",
      "Downloading charset_normalizer-3.4.4-cp314-cp314-win_amd64.whl (107 kB)\n",
      "Downloading cryptography-46.0.3-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 2.1/3.5 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 9.4 MB/s  0:00:00\n",
      "Downloading cffi-2.0.0-cp314-cp314-win_amd64.whl (185 kB)\n",
      "Downloading pillow-12.0.0-cp314-cp314-win_amd64.whl (7.1 MB)\n",
      "   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.1/7.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.9/7.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.5/7.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.1/7.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.1/7.1 MB 8.1 MB/s  0:00:00\n",
      "Downloading pypdfium2-5.0.0-py3-none-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/3.1 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.6/3.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.4/3.1 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 3.9 MB/s  0:00:00\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, Pillow, charset-normalizer, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "\n",
      "   ---------------------------------------- 0/8 [pypdfium2]\n",
      "   ---------------------------------------- 0/8 [pypdfium2]\n",
      "   ---------------------------------------- 0/8 [pypdfium2]\n",
      "   ----- ---------------------------------- 1/8 [pycparser]\n",
      "   ----- ---------------------------------- 1/8 [pycparser]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   ---------- ----------------------------- 2/8 [Pillow]\n",
      "   --------------- ------------------------ 3/8 [charset-normalizer]\n",
      "   -------------------- ------------------- 4/8 [cffi]\n",
      "   -------------------- ------------------- 4/8 [cffi]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------- -------------- 5/8 [cryptography]\n",
      "   ------------------------------ --------- 6/8 [pdfminer.six]\n",
      "   ------------------------------ --------- 6/8 [pdfminer.six]\n",
      "   ------------------------------ --------- 6/8 [pdfminer.six]\n",
      "   ------------------------------ --------- 6/8 [pdfminer.six]\n",
      "   ------------------------------ --------- 6/8 [pdfminer.six]\n",
      "   ----------------------------------- ---- 7/8 [pdfplumber]\n",
      "   ----------------------------------- ---- 7/8 [pdfplumber]\n",
      "   ---------------------------------------- 8/8 [pdfplumber]\n",
      "\n",
      "Successfully installed Pillow-12.0.0 cffi-2.0.0 charset-normalizer-3.4.4 cryptography-46.0.3 pdfminer.six-20251107 pdfplumber-0.11.8 pycparser-2.23 pypdfium2-5.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445c75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.8/18.4 MB 7.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.4/18.4 MB 7.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 6.3/18.4 MB 7.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 9.2/18.4 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 12.3/18.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 13.9/18.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.4/18.4 MB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 14.9/18.4 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 15.5/18.4 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.3/18.4 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.0/18.4 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.0/18.4 MB 5.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.8/18.4 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  18.4/18.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.4/18.4 MB 4.8 MB/s  0:00:03\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318a373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMuPDF-Extraktion fertig: C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6_cut.pdf\"\n",
    "output_txt = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6.txt\"\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "with open(output_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")  # wichtigste Methode\n",
    "\n",
    "        f.write(f\"\\n\\n=== Seite {page_num+1} ===\\n\\n\")\n",
    "        f.write(text)\n",
    "\n",
    "print(\"PyMuPDF-Extraktion fertig:\", output_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54037fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimalparser fertig.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "input_txt = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6.txt\"\n",
    "output_json = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6.json\"\n",
    "\n",
    "def is_title(line):\n",
    "    # extrem simpel: viele Großbuchstaben + kein Section-Start\n",
    "    if re.match(r\"^\\d{1,2}\\.\", line):\n",
    "        return False\n",
    "    upp = sum(c.isupper() for c in line)\n",
    "    low = sum(c.islower() for c in line)\n",
    "    return upp > low and len(line) > 3\n",
    "\n",
    "section_re = re.compile(r\"^(10|[1-9])\\.\\s*(.*)$\")\n",
    "\n",
    "with open(input_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [ln.rstrip(\"\\n\") for ln in f]\n",
    "\n",
    "articles = []\n",
    "current = None\n",
    "i = 0\n",
    "while i < len(lines):\n",
    "    line = lines[i].strip()\n",
    "\n",
    "    # Artikel-Titel\n",
    "    if is_title(line):\n",
    "        if current:\n",
    "            articles.append(current)\n",
    "        current = {\n",
    "            \"name\": line,\n",
    "            \"header\": \"\",\n",
    "            \"sections\": {str(n): \"\" for n in range(1, 11)}\n",
    "        }\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    if not current:\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    # Sectionstart\n",
    "    m = section_re.match(line)\n",
    "    if m:\n",
    "        sec = m.group(1)\n",
    "        rest = m.group(2).strip()\n",
    "\n",
    "        buf = []\n",
    "        if rest:\n",
    "            buf.append(rest)\n",
    "\n",
    "        j = i + 1\n",
    "        while j < len(lines):\n",
    "            nxt = lines[j].strip()\n",
    "            # brich ab bei *einer echten neuen Section*\n",
    "            if section_re.match(nxt):\n",
    "                break\n",
    "            # brich ab bei neuem Titel\n",
    "            if is_title(nxt):\n",
    "                break\n",
    "            buf.append(nxt)\n",
    "            j += 1\n",
    "\n",
    "        # Sammeln\n",
    "        text = \" \".join(buf).strip()\n",
    "        if text == \"\":\n",
    "            text = \"-\"\n",
    "\n",
    "        current[\"sections\"][sec] = text\n",
    "        i = j\n",
    "        continue\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# letztes speichern\n",
    "if current:\n",
    "    articles.append(current)\n",
    "\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(articles, out, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Minimalparser fertig.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506788f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fertig. Artikel: 1207\n",
      "→ C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6_prestructured.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "INPUT = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6.txt\"\n",
    "OUTPUT = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6_prestructured.json\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0. Silbentrennung reparieren\n",
    "# ============================================================\n",
    "\n",
    "def fix_hyphenation(lines):\n",
    "    out = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        stripped = line.rstrip()\n",
    "\n",
    "        # typische Trennung: endet auf \"-\" + nächste Zeile klein\n",
    "        if stripped.endswith(\"-\") and i + 1 < len(lines):\n",
    "            nxt = lines[i + 1].lstrip()\n",
    "            if nxt and nxt[0].islower():\n",
    "                out.append(stripped[:-1] + nxt)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "        out.append(line)\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Titel erkennen\n",
    "# ============================================================\n",
    "\n",
    "def is_article_title(line: str) -> bool:\n",
    "    \"\"\"Ein echter Titel ist ALL CAPS / überwiegend Großbuchstaben.\n",
    "       Dinge wie 's. Strausberg' oder 'ssö Eberswalde' dürfen KEINE Titel sein.\n",
    "    \"\"\"\n",
    "\n",
    "    if not line or len(line.strip()) < 2:\n",
    "        return False\n",
    "\n",
    "    # „s.“, „ssö“, „nö“, „sw“, etc.\n",
    "    if re.match(r\"^(s\\.|ss[öo]\\.|nö\\.|no\\.|so\\.|sw\\.)\\s\", line.lower()):\n",
    "        return False\n",
    "\n",
    "    # Sections sind keine Titel\n",
    "    if re.match(r\"^(10|[1-9])\\.\\s+\", line):\n",
    "        return False\n",
    "\n",
    "    # Titel bestehen meist aus Großbuchstaben + evtl. Klammern oder Apostroph\n",
    "    letters = re.findall(r\"[A-Za-zÄÖÜäöüß']\", line)\n",
    "    if not letters:\n",
    "        return False\n",
    "\n",
    "    # Anteil Großbuchstaben\n",
    "    upper = sum(1 for c in letters if c.isupper())\n",
    "    ratio = upper / len(letters)\n",
    "\n",
    "    # mindestens 60% Großbuchstaben + mindestens 1 Wort > 2 Zeichen\n",
    "    if ratio >= 0.60 and any(len(w) > 2 for w in re.findall(r\"[A-Za-zÄÖÜäöüß']+\", line)):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Section-Erkennung\n",
    "# ============================================================\n",
    "\n",
    "section_re = re.compile(r\"^(10|[1-9])\\.\\s*(.*)$\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Hauptparser\n",
    "# ============================================================\n",
    "\n",
    "def parse_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [ln.rstrip(\"\\n\") for ln in f.readlines()]\n",
    "\n",
    "    # Silbentrennung 2x durchlaufen lassen = robust\n",
    "    for _ in range(2):\n",
    "        lines = fix_hyphenation(lines)\n",
    "\n",
    "    articles = []\n",
    "    current = None\n",
    "\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "\n",
    "    while i < n:\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        if not line:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------\n",
    "        # TITEL\n",
    "        # -------------------------------------------\n",
    "        if is_article_title(line):\n",
    "            # alten Artikel abschließen\n",
    "            if current:\n",
    "                # header zusammenführen\n",
    "                current[\"header\"] = \" \".join(current[\"header_lines\"]).strip()\n",
    "                del current[\"header_lines\"]\n",
    "                articles.append(current)\n",
    "\n",
    "            # neuen Artikel beginnen\n",
    "            current = {\n",
    "                \"name\": line,\n",
    "                \"header\": \"\",\n",
    "                \"header_lines\": [],\n",
    "                \"sections\": {str(k): \"\" for k in range(1, 11)}\n",
    "            }\n",
    "\n",
    "            # Header-Zeilen sammeln\n",
    "            j = i + 1\n",
    "            while j < n:\n",
    "                nxt = lines[j].strip()\n",
    "\n",
    "                if not nxt:\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "                # stop wenn neue section/titel\n",
    "                if section_re.match(nxt):\n",
    "                    break\n",
    "                if is_article_title(nxt):\n",
    "                    break\n",
    "\n",
    "                # alles andere gehört zum Header\n",
    "                current[\"header_lines\"].append(nxt)\n",
    "                j += 1\n",
    "\n",
    "            i = j\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------\n",
    "        # Falls noch kein Artikel offen → skip\n",
    "        # -------------------------------------------\n",
    "        if not current:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------\n",
    "        # SECTION\n",
    "        # -------------------------------------------\n",
    "        m = section_re.match(line)\n",
    "        if m:\n",
    "            sec = m.group(1)\n",
    "            rest = m.group(2).strip()\n",
    "\n",
    "            buf = []\n",
    "            if rest:\n",
    "                buf.append(rest)\n",
    "                k = i + 1\n",
    "            else:\n",
    "                # Folgezeilen sammeln\n",
    "                k = i + 1\n",
    "                while k < n:\n",
    "                    nxt = lines[k].strip()\n",
    "                    if not nxt:\n",
    "                        k += 1\n",
    "                        continue\n",
    "\n",
    "                    if section_re.match(nxt) or is_article_title(nxt):\n",
    "                        break\n",
    "\n",
    "                    buf.append(nxt)\n",
    "                    k += 1\n",
    "\n",
    "            text = \" \".join(buf).strip()\n",
    "            if not text:\n",
    "                text = \"-\"\n",
    "\n",
    "            current[\"sections\"][sec] = text\n",
    "            i = k\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------\n",
    "        # Zeilen, die nicht zu Section gehören → an letzte volle Section\n",
    "        # -------------------------------------------\n",
    "        last_sec = None\n",
    "        for s in range(10, 0, -1):\n",
    "            if current[\"sections\"][str(s)].strip() not in (\"\", \"-\"):\n",
    "                last_sec = str(s)\n",
    "                break\n",
    "\n",
    "        if last_sec:\n",
    "            current[\"sections\"][last_sec] += \" \" + line\n",
    "        else:\n",
    "            current[\"header_lines\"].append(line)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # letzten Artikel hinzufügen\n",
    "    if current:\n",
    "        current[\"header\"] = \" \".join(current[\"header_lines\"]).strip()\n",
    "        del current[\"header_lines\"]\n",
    "        articles.append(current)\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. main()\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    articles = parse_file(INPUT)\n",
    "    with open(OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Fertig. Artikel:\", len(articles))\n",
    "    print(\"→\", OUTPUT)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cf9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: fertig -> C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6_normalized.json Artikel: 1199\n"
     ]
    }
   ],
   "source": [
    "# normalize_json.py\n",
    "# Nimmt das JSON vom Parser und normalisiert Texte (OCR-Fixes, Seite-Marker-Entfernung, Hyphenation-Reparatur),\n",
    "# ohne die wichtigen Single-dashes (Section-Placeholder '-') zu löschen oder Hektar/Morgen-Angaben zu zerstören.\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "INPUT_JSON = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6_prestructured.json\"\n",
    "OUTPUT_JSON = r\"C:\\Users\\daniil\\Desktop\\BRHIKO\\pdf\\HOL_BB_6_normalized.json\"\n",
    "\n",
    "# OCR-ersatzpaare (einfach)\n",
    "REPLACEMENTS = {\n",
    "    \"1c12\": \"16/12\",   # Beispielhafte Anpassung (nur wenn wirklich gewünscht, ggf. entfernen)\n",
    "    \"1c\": \"16\",\n",
    "    \"1ö\": \"16\",\n",
    "    \"  ,\": \",\",\n",
    "    \" ,\": \",\",\n",
    "    \"`\": \"\",\n",
    "    \"·\": \"\",\n",
    "    \"•\": \"\",\n",
    "    \"‚\": \",\",\n",
    "    \"“\": \"\\\"\",\n",
    "    \"”\": \"\\\"\",\n",
    "    \"  \": \" \",\n",
    "}\n",
    "\n",
    "def remove_page_markers(text):\n",
    "    return re.sub(r\"===\\s*Seite\\s*\\d+\\s*===\", \" \", text)\n",
    "\n",
    "def fix_ocr_years(text):\n",
    "    text = re.sub(r'\\b1[BbIl](\\d{2})\\b', lambda m: \"18\" + m.group(1), text)\n",
    "    text = re.sub(r'\\b[Il](\\d{3})\\b', lambda m: \"1\" + m.group(1), text)\n",
    "    text = re.sub(r'\\b(\\d)O(\\d{2})\\b', lambda m: m.group(1) + \"0\" + m.group(2), text)\n",
    "    return text\n",
    "\n",
    "def repair_hyphenation_in_text(text):\n",
    "    # Repariere Silbentrennung über Zeilen (beachte: hier sind Einträge bereits Ein-Zeilen-Strings)\n",
    "    # Beispiel: \"Ablö- s~~g\" -> \"Ablösung\" (sofern Buchstaben folgen)\n",
    "    text = re.sub(r'([A-Za-zÄÖÜäöüß])-\\s*([A-Za-zÄÖÜäöüß])', r'\\1\\2', text)\n",
    "    # Entferne überflüssige Wiederholungen von ~ oder s~~g Artefakte\n",
    "    text = text.replace(\"s~~g\", \"s\")\n",
    "    # Falls versehentlich \"  -\" (space dash) entstanden ist: behalten, aber trimme überflüssige spaces\n",
    "    return text\n",
    "\n",
    "def collapse_spaces_but_keep_single_dash(text):\n",
    "    # wir wollen mehrere Whitespace reduzieren, BUT: token '-' should remain a standalone token if original was single '-'\n",
    "    # normalize whitespace first\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # trim\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_field(text):\n",
    "    if text is None:\n",
    "        return text\n",
    "    # keep original placeholder '-' intact\n",
    "    if text.strip() == \"-\":\n",
    "        return \"-\"\n",
    "\n",
    "    t = text\n",
    "    t = remove_page_markers(t)\n",
    "    t = fix_ocr_years(t)\n",
    "    t = repair_hyphenation_in_text(t)\n",
    "\n",
    "    # perform simple replacements\n",
    "    for a, b in REPLACEMENTS.items():\n",
    "        t = t.replace(a, b)\n",
    "\n",
    "    # collapse spaces while keeping single '-' tokens\n",
    "    t = collapse_spaces_but_keep_single_dash(t)\n",
    "\n",
    "    # final trim\n",
    "    return t.strip()\n",
    "\n",
    "def is_empty_article_keep_header(art):\n",
    "    # Entscheide, ob Artikel lösbar ist. WICHTIG: Wenn es nur header gibt (z. B. \"s. Germandorf\"),\n",
    "    # wollen wir den Artikel BEHALTEN (weil das bedeutende Verweisinfo enthält).\n",
    "    name = (art.get(\"name\") or \"\").strip()\n",
    "    header = (art.get(\"header\") or \"\").strip()\n",
    "    sections = art.get(\"sections\", {})\n",
    "\n",
    "    # Wenn Name fehlt -> drop\n",
    "    if not name:\n",
    "        return True\n",
    "\n",
    "    # Ghost names (Seiten- oder Kapitelüberschriften) droppen\n",
    "    ghost_names = {\"TEIL\", \"INHALT\", \"INDEX\", \"ANHANG\", \"REGISTER\", \"SEITE\", \"KAPITEL\"}\n",
    "    if name.upper() in ghost_names:\n",
    "        return True\n",
    "\n",
    "    # Wenn alle sections leer UND header leer -> drop\n",
    "    all_sections_empty = all((sections.get(str(i), \"\").strip() in (\"\", \"-\")) for i in range(1, 11))\n",
    "    if all_sections_empty and header == \"\":\n",
    "        # But keep if the name itself looks like a genuine place (uppercase and vowel)\n",
    "        if re.search(r\"[AEIOUÄÖÜaeiouäöü]\", name):\n",
    "            # keep it (it may be a \"s. ...\" entry stored as name)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    normalized = []\n",
    "    for art in data:\n",
    "        # normalize header + sections\n",
    "        art[\"header\"] = normalize_field(art.get(\"header\", \"\"))\n",
    "        for k in list(art.get(\"sections\", {}).keys()):\n",
    "            art[\"sections\"][k] = normalize_field(art[\"sections\"].get(k, \"\"))\n",
    "\n",
    "        # remove accidental page markers left in sections (already done in normalize_field)\n",
    "        # Drop only really empty / ghost articles\n",
    "        if not is_empty_article_keep_header(art):\n",
    "            normalized.append(art)\n",
    "        else:\n",
    "            # If it's borderline (name exists but header empty and all sections empty),\n",
    "            # keep if name contains uppercase words typical for places (e.g., \"ABDECKEREI\")\n",
    "            nm = art.get(\"name\",\"\").strip()\n",
    "            if nm and re.match(r'^[A-ZÄÖÜ0-9\\'\\-\\s\\(\\)]+$', nm) and len(nm) > 2:\n",
    "                normalized.append(art)\n",
    "            # else dropped\n",
    "\n",
    "    # optional: de-duplicate exact name/header pairs preserving the first occurrence\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for art in normalized:\n",
    "        key = (art.get(\"name\",\"\").strip().upper(), art.get(\"header\",\"\").strip())\n",
    "        if key in seen:\n",
    "            # attempt gentle merge if different sections present\n",
    "            for prev in out:\n",
    "                if prev.get(\"name\",\"\").strip().upper() == art.get(\"name\",\"\").strip().upper():\n",
    "                    for s in prev[\"sections\"]:\n",
    "                        if prev[\"sections\"][s].strip() in (\"\", \"-\") and art[\"sections\"].get(s,\"\").strip() not in (\"\", \"-\"):\n",
    "                            prev[\"sections\"][s] = art[\"sections\"][s]\n",
    "                    if not prev[\"header\"] and art[\"header\"]:\n",
    "                        prev[\"header\"] = art[\"header\"]\n",
    "                    break\n",
    "            continue\n",
    "        out.append(art)\n",
    "        seen.add(key)\n",
    "\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Normalizer: fertig ->\", OUTPUT_JSON, \"Artikel:\", len(out))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
